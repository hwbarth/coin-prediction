{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lstm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lstm module\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Activation, Input\n",
    "from mlflow.models import infer_signature\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "from keras.optimizers import Adam\n",
    "import os \n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "class Lstm:\n",
    "    def __init__(self, data, ticker, frequency, target, timestamps, model_name, SEQ_LENGTH=100):\n",
    "        self.data = data\n",
    "        self.ticker = ticker\n",
    "        self.frequency = frequency\n",
    "        self.target = target\n",
    "        self.timestamps = timestamps\n",
    "        self.model_name = model_name\n",
    "        self.SEQ_LENGTH = SEQ_LENGTH\n",
    "    \n",
    "    def getData(self):\n",
    "\n",
    "        df = self.data\n",
    "        scaler = MinMaxScaler()\n",
    "        self.scaler = scaler\n",
    "        close_price = df[self.target].values.reshape(-1, 1)\n",
    "\n",
    "        scaled_close = scaler.fit_transform(close_price)\n",
    "        # SEQ_LEN = 100\n",
    "        self.X_train, self.y_train, self.X_test, self.y_test = preprocess(scaled_close, self.SEQ_LENGTH, train_split = 0.95)\n",
    "\n",
    "\n",
    "    # def compileModel(self, opt='adam', loss='mean_squared_error'):\n",
    "    #     #We’re creating a 3 layer LSTM Recurrent Neural Network. We use Dropout with a rate of 20% to combat overfitting during training:\n",
    "\n",
    "    #     # Set your constants\n",
    "    #     DROPOUT = 0.2 \n",
    "    #     WINDOW_SIZE = self.SEQ_LENGTH - 1\n",
    "\n",
    "    #     # Define the model\n",
    "    #     self.model = Sequential()\n",
    "\n",
    "    #     self.model.add(Bidirectional(\n",
    "    #         LSTM(WINDOW_SIZE, return_sequences=True),  # Use LSTM directly\n",
    "    #         input_shape=(WINDOW_SIZE, self.X_train.shape[-1])\n",
    "    #     ))\n",
    "    #     self.model.add(Dropout(rate=DROPOUT))\n",
    "\n",
    "    #     # Add more layers as needed...\n",
    "    #     self.model.add(Dense(1))  # Example output layer\n",
    "\n",
    "    #     # Compile the model\n",
    "    #     self.model.compile(optimizer=opt, loss=loss)\n",
    "\n",
    "    #     # Summary of the model\n",
    "    #     self.model.summary()\n",
    "\n",
    "    #     self.model.add(Activation('linear'))\n",
    "\n",
    "    def compileModel(self, opt='adam', loss='mean_squared_error', learning_rate=0.0001):\n",
    "        # Set your constants\n",
    "        DROPOUT = 0.2 \n",
    "        WINDOW_SIZE = self.SEQ_LENGTH - 1\n",
    "\n",
    "        # Define the model\n",
    "        self.model = Sequential()\n",
    "\n",
    "        # Add Input layer\n",
    "        self.model.add(Input(shape=(WINDOW_SIZE, self.X_train.shape[-1])))\n",
    "\n",
    "        # Add LSTM layer with Bidirectional wrapper\n",
    "        self.model.add(Bidirectional(LSTM(WINDOW_SIZE, return_sequences=True)))\n",
    "\n",
    "        # Add Dropout layer\n",
    "        self.model.add(Dropout(rate=DROPOUT))\n",
    "\n",
    "        # Add more layers as needed...\n",
    "        self.model.add(Dense(1))  # Example output layer\n",
    "\n",
    "        # Define the optimizer with the specified learning rate\n",
    "        if opt == 'adam':\n",
    "            optimizer = Adam(learning_rate=learning_rate)\n",
    "        else:\n",
    "            optimizer = opt  # Use other optimizers if specified\n",
    "\n",
    "        # Compile the model\n",
    "        self.model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "        # Add Activation layer (if necessary, usually added in the Dense layer for the output)\n",
    "        self.model.add(Activation('linear'))  # This line might not be needed since Activation can be in Dense\n",
    "\n",
    "        # Summary of the model\n",
    "        print(self.model.summary())\n",
    "\n",
    "    # def trainModel(self):\n",
    "\n",
    "    #     self.today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    #     # Set the experiment name (optional)\n",
    "    #     mlflow.set_experiment(f\"{self.ticker}_{self.frequency}_{self.target}_experiment\")\n",
    "\n",
    "    #     BATCH_SIZE = 32\n",
    "\n",
    "    #     with mlflow.start_run():\n",
    "    #         # Compile the model\n",
    "    #         # self.model.compile(\n",
    "    #         #     loss='mean_squared_error',\n",
    "    #         #     optimizer='adam'\n",
    "    #         # )\n",
    "\n",
    "    #         # Log parameters\n",
    "    #         mlflow.log_param(\"batch_size\", BATCH_SIZE)\n",
    "    #         mlflow.log_param(\"epochs\", 15)\n",
    "    #         mlflow.log_param(\"optimizer\", \"adam\")\n",
    "\n",
    "    #         history = self.model.fit(\n",
    "    #             self.X_train,\n",
    "    #             self.y_train,\n",
    "    #             epochs=15,\n",
    "    #             batch_size=BATCH_SIZE,\n",
    "    #             shuffle=False,\n",
    "    #             validation_split=0.1\n",
    "    #         )\n",
    "\n",
    "    #         # Log metrics\n",
    "    #         mlflow.log_metric(\"final_loss\", history.history['loss'][-1])\n",
    "    #         mlflow.log_metric(\"final_val_loss\", history.history['val_loss'][-1])\n",
    "\n",
    "    # def saveModel(self):\n",
    "\n",
    "    #     # After your model training and before logging the model\n",
    "    #     signature = infer_signature(self.X_train, self.model.predict(self.X_train))\n",
    "\n",
    "    #     with mlflow.start_run():\n",
    "    #         mlflow.keras.log_model(self.model, \"model\", signature=signature)\n",
    "    #         # Save the model locally\n",
    "    #         self.model.save(f\"models/lstm/{self.ticker}_{self.frequency}_{self.target}_{self.today}.h5\")\n",
    "    def trainModel(self):\n",
    "        self.model_date = datetime.now().date().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        # Set the experiment name (optional)\n",
    "        mlflow.set_experiment(f\"{self.ticker}_{self.frequency}_{self.target}_{self.model_date}_lstm_experiment\")\n",
    "\n",
    "        BATCH_SIZE = 32\n",
    "\n",
    "        with mlflow.start_run():\n",
    "            # Log parameters\n",
    "            mlflow.log_param(\"batch_size\", BATCH_SIZE)\n",
    "            mlflow.log_param(\"epochs\", 15)\n",
    "            mlflow.log_param(\"optimizer\", \"adam\")\n",
    "\n",
    "            # Train the model\n",
    "            history = self.model.fit(\n",
    "                self.X_train,\n",
    "                self.y_train,\n",
    "                epochs=15,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                shuffle=False,\n",
    "                validation_split=0.1\n",
    "            )\n",
    "\n",
    "            # Log metrics\n",
    "            mlflow.log_metric(\"final_loss\", history.history['loss'][-1])\n",
    "            mlflow.log_metric(\"final_val_loss\", history.history['val_loss'][-1])\n",
    "\n",
    "    def saveModel(self):\n",
    "        # Predict on validation/test data\n",
    "        y_pred = self.model.predict(self.X_test)\n",
    "        # Convert predictions to class labels if necessary\n",
    "        y_pred_classes = (y_pred > 0.5).astype(int)  # Example for binary classification\n",
    "\n",
    "        # Calculate accuracy (or any other relevant metrics)\n",
    "        accuracy = accuracy_score(self.y_test, y_pred_classes)\n",
    "\n",
    "        # Create a model directory\n",
    "        model_name = f\"{self.ticker}_{self.frequency}_{self.target}_{self.model_date}\"\n",
    "        model_dir = f\"models/lstm/{model_name}\"\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "        # Save the model as an .h5 file\n",
    "        model_path = os.path.join(model_dir, f\"{model_name}.h5\")\n",
    "        self.model.save(model_path)\n",
    "\n",
    "        # Log the model and metrics with MLflow\n",
    "        with mlflow.start_run():\n",
    "            # mlflow.keras.log_model(self.model, \"model\")\n",
    "            mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "            # Save metrics to a text file\n",
    "            metrics_path = os.path.join(model_dir, \"metrics.txt\")\n",
    "            with open(metrics_path, \"w\") as f:\n",
    "                f.write(f\"Final Accuracy: {accuracy}\\n\")\n",
    "                f.write(\"Classification Report:\\n\")\n",
    "                f.write(classification_report(self.y_test, y_pred_classes))\n",
    "\n",
    "        print(f'LSTM Model saved to {model_path} with accuracy: {accuracy}')\n",
    "\n",
    "    def predictAndUnscale(self):\n",
    "        self.y_pred = self.model.predict(self.X_test)\n",
    "        # Select the last timestep\n",
    "        # Assuming y_hat has shape (batch_size, timesteps, features)\n",
    "        # Select the last timestep for predictions\n",
    "        self.y_pred_last_timestamp = self.y_pred[:, -1, :]  # Make sure y_hat has the expected shape\n",
    "\n",
    "        # Inverse transform the predictions\n",
    "        self.y_pred_inverse = self.scaler.inverse_transform(self.y_pred_last_timestamp)\n",
    "\n",
    "        # Assuming y_test is a 2D array (if you're using a sequence of values)\n",
    "        # If y_test was reshaped correctly when prepared, do this:\n",
    "        self.y_test_inverse = self.scaler.inverse_transform(self.y_test)\n",
    "\n",
    "        if self.y_test.ndim == 3:\n",
    "            self.y_test_last_timestep = self.y_test[:, -1, :]  # Only if y_test is 3D\n",
    "            self.y_test_inverse = self.scaler.inverse_transform(self.y_test_last_timestep)\n",
    "\n",
    "    def setModel(self, model):\n",
    "        self.model = model\n",
    "\n",
    "        \n",
    "\n",
    "def to_sequences(data, seq_len):\n",
    "    d = []\n",
    "\n",
    "    for index in range(len(data) - seq_len):\n",
    "        d.append(data[index: index + seq_len])\n",
    "\n",
    "    return np.array(d)\n",
    "\n",
    "def preprocess(data_raw, seq_len, train_split):\n",
    "\n",
    "    data = to_sequences(data_raw, seq_len)\n",
    "\n",
    "    num_train = int(train_split * data.shape[0])\n",
    "\n",
    "    X_train = data[:num_train, :-1, :]\n",
    "    y_train = data[:num_train, -1, :]\n",
    "\n",
    "    X_test = data[num_train:, :-1, :]\n",
    "    y_test = data[num_train:, -1, :]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           timestamp     open     high      low    close    volume  \\\n",
      "145713  1.718888e+09  66055.7  66055.7  66023.9  66024.0  0.129991   \n",
      "145714  1.718888e+09  66023.9  66023.9  65959.3  65963.8  5.050595   \n",
      "145715  1.718888e+09  65951.0  65951.0  65930.6  65937.3  0.085405   \n",
      "145716  1.718888e+09  65946.4  65948.6  65930.6  65948.6  0.220066   \n",
      "145717  1.718888e+09  65949.6  65949.6  65949.5  65949.5  0.011886   \n",
      "...              ...      ...      ...      ...      ...       ...   \n",
      "291421  1.727740e+09  63203.9  63268.4  63203.9  63263.9  4.582019   \n",
      "291422  1.727740e+09  63264.0  63287.0  63264.0  63287.0  2.031514   \n",
      "291423  1.727741e+09  63287.0  63300.0  63287.0  63300.0  0.737142   \n",
      "291424  1.727741e+09  63300.0  63315.3  63300.0  63306.0  0.317491   \n",
      "291425  1.727741e+09  63306.0  63338.5  63305.9  63338.5  0.336572   \n",
      "\n",
      "                   0             1          2             3  ...  \\\n",
      "145713  1.718888e+09  66097.580386  66005.390  65913.199614  ...   \n",
      "145714  1.718888e+09  66097.619431  66005.340  65913.060569  ...   \n",
      "145715  1.718888e+09  66099.478694  66002.690  65905.901306  ...   \n",
      "145716  1.718888e+09  66100.313804  66001.090  65901.866196  ...   \n",
      "145717  1.718888e+09  66100.671405  66000.695  65900.718595  ...   \n",
      "...              ...           ...        ...           ...  ...   \n",
      "291421  1.727740e+09  63340.589800  63138.330  62936.070200  ...   \n",
      "291422  1.727740e+09  63354.913069  63143.345  62931.776931  ...   \n",
      "291423  1.727741e+09  63367.847077  63147.535  62927.222923  ...   \n",
      "291424  1.727741e+09  63381.232386  63152.025  62922.817614  ...   \n",
      "291425  1.727741e+09  63404.301834  63161.080  62917.858166  ...   \n",
      "\n",
      "        hilberttransformdominantcycleperiod_1  \\\n",
      "145713                              17.974721   \n",
      "145714                              17.988318   \n",
      "145715                              18.083367   \n",
      "145716                              18.237660   \n",
      "145717                              18.365658   \n",
      "...                                       ...   \n",
      "291421                              30.535926   \n",
      "291422                              29.436880   \n",
      "291423                              28.107984   \n",
      "291424                              26.796453   \n",
      "291425                              25.681133   \n",
      "\n",
      "        hilberttransformdominantcyclephase_1  hilberttransformtrendmode_1  \\\n",
      "145713                            186.321162                          1.0   \n",
      "145714                            202.693954                          0.0   \n",
      "145715                            222.323163                          0.0   \n",
      "145716                            243.144939                          0.0   \n",
      "145717                            262.940346                          0.0   \n",
      "...                                      ...                          ...   \n",
      "291421                             47.004691                          0.0   \n",
      "291422                             65.385510                          1.0   \n",
      "291423                             83.057675                          0.0   \n",
      "291424                            103.593381                          0.0   \n",
      "291425                            126.243176                          0.0   \n",
      "\n",
      "        return_2n  return_4n  return_8n  return_16n  return_32n  return_64n  \\\n",
      "145713  -0.001313  -0.001128  -0.002643   -0.002949   -0.010666   -0.013628   \n",
      "145714  -0.000230  -0.000518  -0.001654   -0.001972   -0.009349   -0.013206   \n",
      "145715   0.000185  -0.000419  -0.000816   -0.002296   -0.010299   -0.013604   \n",
      "145716  -0.000288  -0.001448  -0.001325   -0.003438   -0.010780   -0.013771   \n",
      "145717  -0.000603  -0.001516  -0.001516   -0.004596   -0.010908   -0.015080   \n",
      "...           ...        ...        ...         ...         ...         ...   \n",
      "291421   0.000571   0.001179        NaN         NaN         NaN         NaN   \n",
      "291422   0.000300        NaN        NaN         NaN         NaN         NaN   \n",
      "291423   0.000608        NaN        NaN         NaN         NaN         NaN   \n",
      "291424        NaN        NaN        NaN         NaN         NaN         NaN   \n",
      "291425        NaN        NaN        NaN         NaN         NaN         NaN   \n",
      "\n",
      "        return_256n  \n",
      "145713    -0.018099  \n",
      "145714    -0.017172  \n",
      "145715    -0.016643  \n",
      "145716    -0.016759  \n",
      "145717    -0.016323  \n",
      "...             ...  \n",
      "291421          NaN  \n",
      "291422          NaN  \n",
      "291423          NaN  \n",
      "291424          NaN  \n",
      "291425          NaN  \n",
      "\n",
      "[145713 rows x 104 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">198</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">79,992</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">198</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">199</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional_4 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m198\u001b[0m)        │        \u001b[38;5;34m79,992\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m198\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m1\u001b[0m)          │           \u001b[38;5;34m199\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_4 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m1\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">80,191</span> (313.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m80,191\u001b[0m (313.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">80,191</span> (313.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m80,191\u001b[0m (313.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/02 13:12:26 INFO mlflow.tracking.fluent: Experiment with name 'XBTUSD_1_return_8n_2024-11-02_lstm_experiment' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/15\n",
      "\u001b[1m3891/3891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 65ms/step - loss: 0.0120 - val_loss: 7.2513e-04\n",
      "Epoch 2/15\n",
      "\u001b[1m3891/3891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 66ms/step - loss: 0.0010 - val_loss: 3.6322e-04\n",
      "Epoch 3/15\n",
      "\u001b[1m3891/3891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 66ms/step - loss: 6.7188e-04 - val_loss: 3.2253e-04\n",
      "Epoch 4/15\n",
      "\u001b[1m3891/3891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 66ms/step - loss: 6.0181e-04 - val_loss: 3.2138e-04\n",
      "Epoch 5/15\n",
      "\u001b[1m3891/3891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 65ms/step - loss: 5.7302e-04 - val_loss: 3.1453e-04\n",
      "Epoch 6/15\n",
      "\u001b[1m3891/3891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 61ms/step - loss: 5.5195e-04 - val_loss: 3.1248e-04\n",
      "Epoch 7/15\n",
      "\u001b[1m3891/3891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 62ms/step - loss: 5.3457e-04 - val_loss: 3.0533e-04\n",
      "Epoch 8/15\n",
      "\u001b[1m3891/3891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 63ms/step - loss: 5.1934e-04 - val_loss: 3.0009e-04\n",
      "Epoch 9/15\n",
      "\u001b[1m3891/3891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 67ms/step - loss: 5.0596e-04 - val_loss: 2.9625e-04\n",
      "Epoch 10/15\n",
      "\u001b[1m3891/3891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 68ms/step - loss: 4.9379e-04 - val_loss: 2.9157e-04\n",
      "Epoch 11/15\n",
      "\u001b[1m3891/3891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 68ms/step - loss: 4.8356e-04 - val_loss: 2.8750e-04\n",
      "Epoch 12/15\n",
      "\u001b[1m3891/3891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 67ms/step - loss: 4.7374e-04 - val_loss: 2.8457e-04\n",
      "Epoch 13/15\n",
      "\u001b[1m3891/3891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 66ms/step - loss: 4.6458e-04 - val_loss: 2.8222e-04\n",
      "Epoch 14/15\n",
      "\u001b[1m3891/3891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m248s\u001b[0m 64ms/step - loss: 4.5671e-04 - val_loss: 2.8026e-04\n",
      "Epoch 15/15\n",
      "\u001b[1m3891/3891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 61ms/step - loss: 4.4902e-04 - val_loss: 2.7757e-04\n"
     ]
    }
   ],
   "source": [
    "ticker = \"XBTUSD\"\n",
    "frq = \"1\"\n",
    "target = \"return_8n\"\n",
    "# parser = argparse.ArgumentParser(description='Process cryptocurrency data.')\n",
    "# parser.add_argument('ticker', type=str, help='The cryptocurrency ticker symbol (e.g., BTC, ETH)')\n",
    "# parser.add_argument('frequency', type=int, help='The frequency of data points (e.g., 1 for daily, 7 for weekly)')\n",
    "# parser.add_argument('target', type=str, help='target return period (e.g return_n8 (return for period n + 8), 2^i')\n",
    "\n",
    "\n",
    "# args = parser.parse_args()\n",
    "# ticker = args.ticker\n",
    "# frq = args.frequency\n",
    "# target = args.target\n",
    "\n",
    "df = pd.read_csv(f\"data/silver_prices/{ticker}_{frq}_silver.csv\")\n",
    "\n",
    "df = df[-int((len(df)/2)):]\n",
    "\n",
    "print(df)\n",
    "timestamps = list(df['timestamp'])\n",
    "model_date = datetime.now().date().strftime(\"%Y-%m-%d\")\n",
    "#\n",
    "model_name = f\"{ticker}_{frq}_{target}_{model_date}\"\n",
    "model_path = \"models/random-forest\"+model_name\n",
    "\n",
    "lstm = Lstm(df, ticker, frq, target, timestamps, model_name, SEQ_LENGTH=100)\n",
    "\n",
    "lstm.getData()\n",
    "lstm.compileModel()\n",
    "lstm.trainModel()\n",
    "# lstm.saveModel()\n",
    "# lstm.predictAndUnscale()\n",
    "# preds = lstm.y_pred_inverse\n",
    "# real = lstm.y_test_inverse\n",
    "# data = {\n",
    "#     \"timestamps\": timestamps,\n",
    "#     \"predicted\":preds,\n",
    "#     \"real\":real,\n",
    "# }\n",
    "# result = pd.DataFrame(data)\n",
    "# result.to_csv(f\"models/model-output/lstm/{model_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input y_true contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lstm\u001b[38;5;241m.\u001b[39msaveModel()\n\u001b[1;32m      2\u001b[0m lstm\u001b[38;5;241m.\u001b[39mpredictAndUnscale()\n\u001b[1;32m      3\u001b[0m preds \u001b[38;5;241m=\u001b[39m lstm\u001b[38;5;241m.\u001b[39my_pred_inverse\n",
      "Cell \u001b[0;32mIn[14], line 183\u001b[0m, in \u001b[0;36mLstm.saveModel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m y_pred_classes \u001b[38;5;241m=\u001b[39m (y_pred \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)  \u001b[38;5;66;03m# Example for binary classification\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# Calculate accuracy (or any other relevant metrics)\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_test, y_pred_classes)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# Create a model directory\u001b[39;00m\n\u001b[1;32m    186\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfrequency\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/crypto/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/crypto/lib/python3.11/site-packages/sklearn/metrics/_classification.py:231\u001b[0m, in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    229\u001b[0m xp, _, device \u001b[38;5;241m=\u001b[39m get_namespace_and_device(y_true, y_pred, sample_weight)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[1;32m    232\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniforge3/envs/crypto/lib/python3.11/site-packages/sklearn/metrics/_classification.py:104\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    102\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred)\n\u001b[1;32m    103\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[0;32m--> 104\u001b[0m type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    105\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    107\u001b[0m y_type \u001b[38;5;241m=\u001b[39m {type_true, type_pred}\n",
      "File \u001b[0;32m~/miniforge3/envs/crypto/lib/python3.11/site-packages/sklearn/utils/multiclass.py:398\u001b[0m, in \u001b[0;36mtype_of_target\u001b[0;34m(y, input_name)\u001b[0m\n\u001b[1;32m    396\u001b[0m     data \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;28;01mif\u001b[39;00m issparse(y) \u001b[38;5;28;01melse\u001b[39;00m y\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39many(data \u001b[38;5;241m!=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(data, \u001b[38;5;28mint\u001b[39m)):\n\u001b[0;32m--> 398\u001b[0m         _assert_all_finite(data, input_name\u001b[38;5;241m=\u001b[39minput_name)\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontinuous\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m suffix\n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m# Check multiclass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/crypto/lib/python3.11/site-packages/sklearn/utils/validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m _assert_all_finite_element_wise(\n\u001b[1;32m    124\u001b[0m     X,\n\u001b[1;32m    125\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[1;32m    126\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[1;32m    127\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[1;32m    128\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    129\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    130\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/crypto/lib/python3.11/site-packages/sklearn/utils/validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    171\u001b[0m     )\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input y_true contains NaN."
     ]
    }
   ],
   "source": [
    "lstm.saveModel()\n",
    "lstm.predictAndUnscale()\n",
    "preds = lstm.y_pred_inverse\n",
    "real = lstm.y_test_inverse\n",
    "data = {\n",
    "    \"timestamps\": timestamps,\n",
    "    \"predicted\":preds,\n",
    "    \"real\":real,\n",
    "}\n",
    "result = pd.DataFrame(data)\n",
    "result.to_csv(f\"models/model-output/lstm/{model_name}.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cbcf5f2c92c3813cf756da811d2108a7473fed3b7b8c64283edd03593b1ba677"
  },
  "kernelspec": {
   "display_name": "Python 3.11.0 ('crypto')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
